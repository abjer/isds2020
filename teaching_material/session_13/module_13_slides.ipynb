{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 14:\n",
    "## Supervised learning, part 3\n",
    "\n",
    "*Andreas Bjerre-Nielsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "machine learning for social scientists\n",
    "1. [measures for classification](#Measures-for-classification)  \n",
    "1. [nested cross validation](#Nested-cross-validation)  \n",
    "1. [non-linear ML](#Non-linear-ML)\n",
    "  -  [tree based models](#Tree-based-models)\n",
    "  -  [neural networks](#Neural-networks)\n",
    "1. [machine learning for social scientists](#ML-for-social-science)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vaaaamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('default') # set style (colors, background, size, gridlines etc.)\n",
    "plt.rcParams['figure.figsize'] = 10, 4 # set default size of plots\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## With great power ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### comes great responsibility..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You have been suffering a lot with implementing estimators... why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you don't know what is going on you are likely to apply erroneously.\n",
    "- So very important although you don't use in the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measures for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (1)\n",
    "\n",
    "We measure the accaracy as the rate of true predictions, i.e. $$ACC=\\frac{TP+TN}{TP+TN+FP+FN}=\\frac{True}{True+False}$$\n",
    "\n",
    "where our measures are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_08.png' alt=\"Drawing\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (2)\n",
    "\n",
    "Some powerful measures:\n",
    "\n",
    "- Precision: share of predicted positive that are true\n",
    "    - PRE = $\\frac{TP}{TP+FP}$    \n",
    "    - = true positive rate \n",
    "- Recall: share of actual positive that are true    \n",
    "   - REC = $\\frac{TP}{TP+FN}=\\frac{TP}{AP}$ \n",
    "   - = 1- false negative rate\n",
    "- F1: mix recall and precision: $\\frac{2\\cdot PRE\\cdot REC}{PRE+ REC}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (3)\n",
    "\n",
    "Classification models provide a predicted likelihood of being in the class or not:\n",
    "- Receiver Operating Characteristic (ROC) curve by varying thresholds for predicted true.\n",
    "    - ROC is a *theoretical* measure of model performance based on probabilities.\n",
    "    - AUC: Area Under the (ROC) Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_10.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nested cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model validation does not consider that we are also tuning hyperparameters:\n",
    "  - Leads too overfitting (Varma & Simon 2006; Cawley, Talbot 2010).\n",
    "- Solution is **nested cross validation**.\n",
    "  - Validation step should not be modelled as 1) train; 2) test.\n",
    "  - Better way is 1) model selection: train, validate; 2) test.\n",
    "  - Implement as pp 204-205 in Python for Machine Learning:\n",
    "      - first inner loop: `GridSearchCV` \n",
    "      - second outer loop: `cross_val_score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (2)\n",
    "We can apply cross-val. at two levels:\n",
    "- the outer level, i.e. split into test-dev. split\n",
    "- the inner level, i.e. split dev. data into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_07.png' alt=\"Drawing\" style=\"width: 650px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-linear ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Success of machine learning\n",
    "*Are linear models the best performing models?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "George E. P. Box: All models are wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But some are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evidence\n",
    "- Sometimes linear model are the best \n",
    "- But there are many others that in general perform better\n",
    "- They can find patterns that non-linear models cannot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Success of machine learning\n",
    "*Are linear models the best performing models?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Success of machine learning (2)\n",
    "*What do we call models that can fit any pattern?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Universal approximators. \n",
    "    - We can also make input non-linear using `PolynomialFeatures` of any order.\n",
    "        - Follows from iterative Taylor expansion\n",
    "        - Problem?\n",
    "- These are very powerful tools.\n",
    "    - Example of recognizing characters and digits in handwriting (MNIST data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A hierarchal structure \n",
    "*What does a `decision tree` look like?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch03/images/03_17.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample splitting (1)\n",
    "*Suppose we have data like below, we are interested in predicting criminal*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Criminal':[1]*5+[0]*10,\n",
    "                   'From Jutland':np.random.randint(0,2,15),                   \n",
    "                   'Parents together':[0]*4+[1]*10+[0],\n",
    "                   'Parents unemployed':[1]*7+[0]*8})\n",
    "print(df.sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample splitting (2)\n",
    "*Let's try to split by variables and see whether it helps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criminal                               0    1\n",
      "Parents together Parents unemployed          \n",
      "0                0                   1.0  NaN\n",
      "                 1                   NaN  4.0\n",
      "1                0                   7.0  NaN\n",
      "                 1                   2.0  1.0\n"
     ]
    }
   ],
   "source": [
    "my_split = df\\\n",
    "            .groupby(['Parents together', 'Parents unemployed'])\\\n",
    "            .Criminal\\\n",
    "            .value_counts()\\\n",
    "            .unstack(level=-1)\\\n",
    "# Parents together, Parents unemployed\n",
    "print(my_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample splitting (3)\n",
    "*What might a tree structure look like?*\n",
    "\n",
    "- Parents together: Yes > Not criminal    \n",
    "<br/><br/>        \n",
    "- Parents together: No\n",
    "    - Parents unemployed: Yes > Criminal\n",
    "    - Parents unemployed: No > Not criminal        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improving decision trees\n",
    "*What can we conclude about the decision trees?*\n",
    "\n",
    "- Can fit anything ~ Universal Approximation\n",
    "    - *little* underfitting (~low bias)\n",
    "    - **LARGE** overfitting (~large variance)\n",
    "    \n",
    "`random forest` improves on decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks (1)\n",
    "*I have forgotten, what was Adaline?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch12/images/12_01.png' alt=\"Drawing\" style=\"width: 900px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks (2)\n",
    "*Why are neural networks called deep learning?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch12/images/12_02.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks (3)\n",
    "*So learning about the Perceptron and Adaline actually has value?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, lot's of value: these are the neurons of neural networks. \n",
    "\n",
    "In other words, they are fundamental building blocks for doing deep learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks (4)\n",
    "*How are neural networks different from simply using polynomial features?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Cheng et al.](https://arxiv.org/abs/1806.06850) that neural nets and polynomial expansion are essentially the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are however some differences:\n",
    "- Hidden layer can be a lot smaller than all possible combinations \n",
    "    - Small picture (28x28 pixels) with 3 color channels  have more 13 billion combinations.\n",
    "- It uses non-linear activation functions. \n",
    "- A neural network with one hidden layer has universal approximation.\n",
    "    - This corresponds to quadratic if linear.\n",
    "- In practice they perform really well, especially on non-linear data\n",
    "    - Computer vision: recognizing characters, content in images\n",
    "    - Natural Language Processing (NLP): parsing text and speech data\n",
    "    - Much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Universal approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universal approximators (1)\n",
    "*Are decision trees the only universal approximators?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No there are also **kernel based** ones.\n",
    "\n",
    "- K Nearest Neighbors:\n",
    "    - Approximate by taking average/mode from K nearest neighbors\n",
    "        - Need standardization\n",
    "    - Can also be used for interpolated local measures \n",
    "        - (weather, pollution, house prices etc.)\n",
    "    - Not good with high dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universal approximators (2)\n",
    "*What can these these approximators be used for?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reduce bias (underfitting)\n",
    "- Must be careful we do not overfit (control bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Can I get an overview of them?*\n",
    "- Kernel methods, e.g. nearest neighbor\n",
    "- Neural networks (1+ hidden layer, deep learning)\n",
    "- Polynomial inputs\n",
    "- Tree based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universal approximators (3)\n",
    "*Can we use these methods?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, they all come off the shelf with `sklearn`.\n",
    "- E.g. `from sklearn.ensemble import RandomForestClassifier`\n",
    "\n",
    "For neural networks that have more hidden layers (deep learning) you need new packages:\n",
    "- We recommend looking at either `pytorch` or using `keras` (which uses `tensorflow`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Should we use these methods in the exam of this course?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML for social science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML for social science (1): testing predictive power\n",
    "\n",
    "ML helps us with making predictive models: \n",
    "\n",
    "- Assess the performance of our models\n",
    "- Choose the parameters that help estimate the best performing model \n",
    "\n",
    "Can we use ML to help us clarify whether a new feature set is relevant for prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML for social science (2): new data\n",
    "\n",
    "Machine learning can help us *'fill in the blanks'* and impute missing data\n",
    "\n",
    "Input: Google Street View\n",
    "- Infer neighborhood socioeconomic status (Naik, Raskar, Hidalgo 2016)\n",
    "\n",
    "Input: Cell phone data\n",
    "- Inferring poverty. (Blumenstock, Cadamuro, On 2015)\n",
    "- Inferring mode of transportation. (Bjerre-Nielsen et al. 2019)\n",
    "- Sleep (Cuttone et al. 2017)\n",
    "\n",
    "Facebook data (likes, way of writing, town) can help infer\n",
    "- personality and demographics (Cambridge Analytica); socioeconomic status; current mood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML for social science (3): better policy targetting\n",
    "\n",
    "Social and medical scientists are often involved in policies aimed at: \n",
    "- alleviating poverty, decrease drop-out, crime etc.\n",
    "\n",
    "Efficacy of these programs requires targetting of individuals:\n",
    "- who is most poor, who is most at risk of dropping out? dying?\n",
    "\n",
    "Kleinberg et al. 2015 show that mortality from surgery can be predicted in advance.\n",
    "- save billions of $ and not cause pain of surgery\n",
    "\n",
    "Other policies: \n",
    "- should we prescribe opioids to you (what is your risk of addiction?)\n",
    "- should we audit your firm for VAT and tax review (how much do we predict you cheat)\n",
    "- should we admit you to this education? (what is your dropout risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML for social science (4): improving econometrics\n",
    "\n",
    "Many econometric methods try to establish causality:\n",
    "\n",
    "- applications for \n",
    "    - instrument variables (Hartford et al. 2017; Bj√∏rn 2018)\n",
    "    - matching (Wager, Athey 2017)\n",
    "    - linear models with many covariates (see work by Hansen, Belloni, Chernozhukov)\n",
    "- models for matching, instrument variables have a prediction problem built-in\n",
    "    - can be enhanced with machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML for social science (5): decision problems and game theory\n",
    "\n",
    "We can solve decision problems and games using reinforcement learning\n",
    "- uses neural networks to teach \"agent\" to play game\n",
    "    - learn to play computer games, poker, etc.\n",
    "- solve problems where game theory is intractable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outro \n",
    "\n",
    "There are amazing resources for you to keep learning, online and offline.\n",
    "\n",
    "@ in Denmark.\n",
    "- Center for Social Data Science\n",
    "  - Advanced courses (replace Topics in Social Data Science)\n",
    "      - ASDS1: tree based models for prediction and statistics; network inference\n",
    "      - ASDS2: unstructured data; (text, image), neural networks\n",
    "  - Seminar in [econometrics and machine learning](https://kurser.ku.dk/course/a%C3%98kk08386u/2019-2020) this fall [booked out]\n",
    "  - A new education next year [more info follows]\n",
    "- More courses are taught in machine learning at CS dept. (DIKU), DTU, ITU\n",
    "\n",
    "- Michael Nielsen: deep learning neural networks\n",
    "\n",
    "@ online: coursera, edX, DataCamp, MIT open courseware, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Everyone freeze!\n",
    "\n",
    "### Please run the course evaluation now (<5 min)\n",
    "\n",
    "- Evaluate our actions: \n",
    "  - What was good, what was not good\n",
    "\n",
    "- Please evaluate: \n",
    "  - our teaching: did I make myself clear? was I too fast? what about Snorre and David?\n",
    "  - the material (lectures, exercises, books)\n",
    "  - autograding\n",
    "  - the quizzes (those that worked)\n",
    "  - machine learning curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The end\n",
    "[Return to agenda](#Agenda)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
