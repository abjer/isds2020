{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6054046183e895b95d96b61b4fcc426",
     "grade": false,
     "grade_id": "cell-c4be203e1da1aa63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mandatory Assignment 2\n",
    "\n",
    "This is the last of three mandatory assignments which must be completed during the course. Note that you only need to pass 2 out of 3 assignments to be eligible for the exam.\n",
    "\n",
    "\n",
    "First some practical information:\n",
    "\n",
    "* When is the assignment due?: **23:59, Thursday, August 20, 2020.**\n",
    "* Should i work with my group?: **Yes**. In particular, you should **only hand in 1 assignment per group**.\n",
    "\n",
    "The assignment consists of problems from some of the exercise sets that you have solved so far.\n",
    "\n",
    "**Note**: It is important that you submit your edited version of THIS [notebook](https://fileinfo.com/extension/ipynb#:~:text=An%20IPYNB%20file%20is%20a,Python%20language%20and%20their%20data.) as a .ipynb file and nothing else. Do not copy your answers into another notebook that you have made. Do not submit your answers as a pdf. Do not convert the notebook to json and then submit that etc. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#RUN THIS CELL AND USE THE DATA TO SOLVE THE NEXT EXERCISES#\n",
    "############################################################\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac = 1, random_state = 3)\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "\n",
    "#EDA before modelling \n",
    "#Do the two species look perfectly linear seperable in any of the plots? \n",
    "ax = sns.pairplot(iris, hue=\"species\", palette=\"husl\", diag_kws = {'shade': False})\n",
    "ax.fig.subplots_adjust(top=0.95)\n",
    "ax.fig.suptitle('Pairplot of features for the virginica and versicolor species', fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# A very simple deterministic test-train split \n",
    "Xtrain = X[:70]\n",
    "ytrain = y[:70]\n",
    "\n",
    "Xtest = X[70:]\n",
    "ytest = y[70:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3149e9ffd94b46b0ece95ad76522c009",
     "grade": false,
     "grade_id": "cell-2549517e368b0847",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 10:\n",
    "\n",
    "> **Ex. 10.1.5:** Write a function named `perceptronEpoch` whichs loops over the training data (both X and y) using `zip`. For each row in the data, update the weights according to the perceptron rule (remember to update the bias in `w[0]`!). Set $\\eta = 0.1$.\n",
    ">\n",
    "> Make sure the loop stores the total number of prediction errors encountered underways in the loop by creating an `int` which is incremented whenever you update the weights. \n",
    ">\n",
    ">> _Hint:_ your function should return the updated weights, as well as the number of errors made by the perceptron.\n",
    ">\n",
    ">> _Hint:_ The following code block implements the function in _pseudo_code (it wont run, but serves to communicate the functionality).\n",
    ">> ```\n",
    ">> function perceptronEpoch(X, y, W, eta):\n",
    ">>    set errors = 0\n",
    ">>\n",
    ">>    for each pair xi, yi in zip(X,y) do:\n",
    ">>        set update = eta * (yi - predict(xi, W))\n",
    ">>        set W[1:] = W[1:] + update * xi\n",
    ">>        set W[0] = W[0] + update\n",
    ">>        set errors = errors + int(update != 0) \n",
    ">>\n",
    ">>    return W, errors\n",
    ">> ```\n",
    ">\n",
    "> *Bonus:* If you completed the previous bonus exercise (for 10.1.4), calculate the accuracy on training data using the updated weights as input in the predict function. Any progress yet?\n",
    "\n",
    "\n",
    "You can use the following functions:\n",
    "\n",
    "```python\n",
    "def random_weights(location = 0.0, scale = 0.01, seed = 1):\n",
    "    # Init random number generator\n",
    "    rgen = np.random.RandomState(seed)\n",
    "    w = rgen.normal(loc=location, scale=scale, size= 1 + X.shape[1])\n",
    "    \n",
    "    return w\n",
    "\n",
    "def net_input(X, W): \n",
    "    return np.dot(X, W[1:]) + W[0]   # Linear product X'W + bias\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    linProd = net_input(X, W)\n",
    "    return np.where(linProd >= 0.0, 1, -1)    # 1(linProd > 0)\n",
    "```\n",
    "\n",
    ">\n",
    "> Make sure your `perceptronEpoch` function takes the arguments `X, y, W, eta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4ac154089454ee1e6abf0eb76098654",
     "grade": false,
     "grade_id": "ex_10_1_5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# def perceptronEpoch(X, y, W, eta = 0.1):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c38e484db7165b25452f29e4091a14d2",
     "grade": true,
     "grade_id": "ex_10_1_5_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w, e = perceptronEpoch(Xtrain, ytrain, random_weights(), 0.1)\n",
    "assert len(w) == 5\n",
    "assert isinstance(e, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "765a1c71842cb5d34f340412f465295f",
     "grade": false,
     "grade_id": "cell-59488e40ebf2f946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 10.1.6:** Write a function named `Perceptron` which repeat the updating procedure (calls the function) you constructed in 11.1.5 for `n_iter` times by packing the whole thing in a loop. Make sure you store the number of errors in each iteration in a list. \n",
    ">\n",
    "> Plot the total errors after each iteration in a graph with the number of epochs along the x-axis and errors along the y-axis.\n",
    ">\n",
    ">> _Hint:_ Make sure you dont reset the weights after each iteration.\n",
    ">\n",
    ">> _Hint:_ Once again some pseudocode:\n",
    ">> ```\n",
    ">> function Perceptron(X, y, n_iter):\n",
    ">>     set eta = 0.1\n",
    ">>     set weights = random_weights()\n",
    ">>     set errorseq = list()\n",
    ">>\n",
    ">>     for each _ in range(n_iter):\n",
    ">>         weights, e = f(X, y, W, eta) \n",
    ">>         errorseq.append(e)\n",
    ">>\n",
    ">>     return weights, errorseq\n",
    ">> ```\n",
    "\n",
    "Please make sure that your function is named `Perceptron` and takes the arguments `X, y, n_iter, eta`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ef4002b42125fae832a3943b4dce73d",
     "grade": false,
     "grade_id": "ex_10_1_6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "497615beb2846323cbb1347cc3c4dc68",
     "grade": true,
     "grade_id": "ex_10_1_6_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "trained_w, errorseq = Perceptron(Xtrain, ytrain, 50, 0.1)\n",
    "assert len(trained_w) == 5\n",
    "assert len(errorseq) == 50\n",
    "assert all(isinstance(i, int) for i in errorseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "287e32174bd33a71246c01ed5bd7c944",
     "grade": false,
     "grade_id": "cell-99668841f5921403",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 11:\n",
    "\n",
    "### First part of Exercise Set 11: Implementing and evaluating the gradient decent for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#RUN THIS CELL AND USE THE DATA TO SOLVE THE NEXT EXERCISES#\n",
    "############################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.exceptions import DataConversionWarning; import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Load the example tips dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "#Get dummies \n",
    "tips_num = pd.get_dummies(tips, drop_first=True)\n",
    "\n",
    "#Define feature matrix X and target vector y \n",
    "X = tips_num.drop('tip', axis = 1)\n",
    "y = tips_num['tip']\n",
    "\n",
    "\n",
    "#NOTE: It is important that the random_state parameter is set equal to 1\n",
    "#to ensure that the same sequence of random numbers are generated as in the solution notebook\n",
    "#Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.5,\n",
    "                                                    random_state=1)\n",
    "\n",
    "#Standardize data - note that we only fit the standard scaler on X_train and use it on X_train, X_test. \n",
    "norm_scaler = StandardScaler().fit(X_train) \n",
    "X_train = norm_scaler.transform(X_train) \n",
    "X_test = norm_scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18fcea1959382b632ae5b6c0a3f445a9",
     "grade": false,
     "grade_id": "cell-3582848e2d16da20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 11.1.5**: Make a function to update the weights given input target `y`, input features `X` and input weights `w` as well as learning rate, $\\eta$, i.e. greek `eta`. Name the function `update_weight`. You should use matrix multiplication.\n",
    "\n",
    "\n",
    "You can use the following functions inside `update_weight`:\n",
    "\n",
    "```python\n",
    "def net_input(X, w):    \n",
    "    ''' Computes the matrix product between X and w. Note that\n",
    "    X is assumed not to contain a bias/intercept column.'''\n",
    "    return np.dot(X, w[1:]) + w[0]   # We have to add w_[0] separately because this is the constant term.\n",
    "                                        # We could also have added a constant term (columns of 1's to X_ and multipliced it to all of w_)\n",
    "\n",
    "def compute_error(y, X, w):\n",
    "    return y - net_input(X, w)\n",
    "    \n",
    "```\n",
    "\n",
    ">\n",
    "> Make sure your function takes the arguments `X, y, W, eta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc0435fb0fbd7b43b24b0f7bfdbbfc50",
     "grade": true,
     "grade_id": "ex_11_1_5_test",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53755c4f714400a835486438ea0aca45",
     "grade": false,
     "grade_id": "cell-f840d5f2932b1d25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 11.1.6**: Use the code below to initialize the weights in the variable `w` at zero given feature set `X`. Notice how we include an extra weight that includes the bias term. Set the learning rate `eta` to 0.001. Make a loop with 50 iterations where you iteratively apply your weight updating function. \n",
    "\n",
    ">```python\n",
    "w = np.zeros(1+X.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ce31cfd31b3acccea4a0c9b3f49172a",
     "grade": false,
     "grade_id": "ex_11_1_6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b2f0abea5adf66eb8d1e6d660008c36",
     "grade": true,
     "grade_id": "ex_11_1_6_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(np.sum(w)) == 4\n",
    "assert any(np.round(w, 2) == [ 2.93,  0.89,  0.1 ,  0.07,  0.08,  0.05,  0.  , -0.04, -0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9aba87fc18183c69edf6a33b319b338",
     "grade": false,
     "grade_id": "cell-a6898fa843a456ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Second part of Exercise Set 11: Modelling houseprices\n",
    "In this example we will try to predict houseprices using a lot of variable (or features as they are called in Machine Learning). We are going to work with Kaggle's dataset on house prices, see information [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), which also is to be found in `sklearn.datasets`. Kaggle is an organization that hosts competitions in building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#RUN THIS CELL AND USE THE DATA TO SOLVE THE NEXT EXERCISES#\n",
    "############################################################\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)    #note random_state=1 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "735264c7e229193c91c3c193466054af",
     "grade": false,
     "grade_id": "cell-6f125b68cf8cebb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex.11.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? Write 2 sentences. \n",
    ">\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  Write 1 sentence. \n",
    ">\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    ">\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i).\n",
    "\n",
    "Name you transformed training data set `X_train2` and your test data set `X_test2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42bb99fbca7f54fd2d31dda42b9c0d42",
     "grade": false,
     "grade_id": "ex_11_2_1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06ef83f7e757f29f1ea6bf80424d544d",
     "grade": true,
     "grade_id": "ex_11_2_1_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(X_train2.mean(axis=0).sum()) == 0\n",
    "assert round(np.std(X_train2, axis=0).sum()) == 83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2e07428858e46e22c0bcc98947f8339",
     "grade": false,
     "grade_id": "cell-e28611963fcc9e39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex.11.2.2**: Construct a list of 20 $\\lambda$ values in the range from $10^{-4}$ to $10^4$ to be used in the Lasso model. For each $\\lambda$ estimate the Lasso model on the rescaled train data, calculate the _Root Mean_ Squared Error (RMSE) for the rescaled train data, `X_train2`, and the _Root Mean_ Squared Error (RMSE) for the rescaled test data, `X_test2`. For each iteration store the given $\\lambda$ and the calculated RMSEs for the rescaled train and test data in a list named `output`. \n",
    "\n",
    "> *Hint*: use `np.logspace(-4, 4, 20)` to create the list of lambdas to loop. \n",
    "\n",
    "> *Hint*: You can use the following code from scikit-learn to compute the _mean_ squared error.\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "mse(y_pred, y_true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f99d24101881a1dabc22d222334de44e",
     "grade": false,
     "grade_id": "ex_11_2_2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13a381a0d103e862d0a9d88e7eacc8d2",
     "grade": true,
     "grade_id": "ex_11_2_2_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert any(np.sort(np.round(np.mean(output, axis=0))) == [1.0, 2.0, 806.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7655982718e884638d84a4e3f2c6de28",
     "grade": false,
     "grade_id": "cell-620ee97ca95a84f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex.11.2.3**: Make a plot with the lambdas on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2815a795b369dd07baae6b28abf23751",
     "grade": true,
     "grade_id": "ex_11_2_3",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "614c0b0ccb6cb7dfa1a4e0de63572e55",
     "grade": false,
     "grade_id": "cell-df90d5afe2967903",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 12:\n",
    "\n",
    "> **Ex. 12.1.4:**\n",
    "Run a Lasso regression using the Pipeline from `Ex 12.1.3` which is given in the code cell below. In the outer loop searching through the 12 lambda values as specified below. \n",
    "In the inner loop make *5 fold cross validation* on the selected model and store the average MSE for each fold. Which lambda, from the selection below, gives the lowest test MSE?\n",
    " ```python \n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "```\n",
    " *Hint:* `KFold` in `sklearn.model_selection` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77d94c0532a0c5dd15d34ca1aa9d745a",
     "grade": true,
     "grade_id": "ex_12_1_4",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Pipeline from Ex. 12.1.3\n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),                           \n",
    "                           StandardScaler(),\n",
    "                           Lasso(random_state=1))\n",
    "\n",
    "#The feature matrix X and target vector y are defined \n",
    "# as below the code cell \"Second part of Exercise Set 11: Modelling houseprices\". \n",
    "\n",
    "#Split X, y into development (2/3) and test data (1/3).\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)    \n",
    "\n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0432d7d18e24fc7f4244f2f0e58689bf",
     "grade": false,
     "grade_id": "cell-878b42b675168f8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Session 14:\n",
    "\n",
    "> **Ex. 1**: Make a preprocessing function that takes a single string and:\n",
    "\n",
    "        1. Lowercases the words.\n",
    "        2. Splits the text into words (tokenize).\n",
    "        3. Either stems or lemmatizes words.\n",
    " \n",
    ">Feel free to add more steps of preprocessing. For example stopword removal or removal of what seems to be HTML elements (such as \"< br/>\") in the text, and removal of punctuation, and handling of emoticons as in the textbook.\n",
    "\n",
    "> Apply the function to the string `movie_review_sample` in the code cell below. Print the processed string. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8e793e7b069eafab13ecd65420adb51",
     "grade": true,
     "grade_id": "ex_14_1",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import string # for efficient operations with strings\n",
    "import nltk   # NLTK: A basic, popular NLP package. \n",
    "\n",
    "#example string to apply your function on \n",
    "movie_review_sample = \"Forbidden Siren is based upon the Siren 2 Playstation 2 (so many 2s) game. Like most video game turned movies, I would say the majority don't translate into a different medium really well. And that goes for this one too, painfully.<br /><br />There's a pretty long prologue which explains and sets the premise for the story, and the mysterious island on which a writer (Leo Morimoto) and his children, daughter Yuki (Yui Ichikawa) and son Hideo (Jun Nishiyama) come to move into. The villagers don't look all too friendly, and soon enough, sound advice is given about the siren on the island, to stay indoors once the siren starts wailing.<br /><br />Naturally and slowly, things start to go bump, and our siblings go on a mission beating around the bush to discover exactly what is happening on this unfriendly island with its strange inhabitants. But in truth, you will not bother with what's going on, as folklore and fairy tales get thrown in to convolute the plot even more. What was really pushing it into the realm of bad comedy are its unwittingly ill-placed-out-of-the-norm moments which just drew pitiful giggles at its sheer stupidity, until it's explained much later. It's one thing trying to come up and present something smart, but another thing doing it convincingly and with loopholes covered.<br /><br />Despite it clocking in under 90 minutes - I think it's a horror movie phenomenon to have that as a runtime benchmark - it gives that almost two hour feel with its slow buildup to tell what it wants to. Things begin to pick up toward the last 20 minutes, but it's a classic case of too little too late.<br /><br />What saves the movie is how it changes tack and its revelation at the end. Again this is a common device used to try and elevate a seemingly simple horror movie into something a little bit extra in the hope of wowing an audience. It turned out rather satisfactorily, but leaves a bad aftertaste as you'll feel cheated somewhat. There are two ways a twist will make you feel - it either elevates the movie to a memorable level, or provides you with that hokey feeling. Unfortunately Forbidden Siren belonged more to the latter.<br /><br />The saving grace will be its cinematography with its use of light, shadows and mirrors, but I will be that explicit - it's still not worth the time, so better to avoid this.\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
